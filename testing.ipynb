{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ujjwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ujjwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Ujjwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from textstat.textstat import textstatistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rise of telemedicine and its Impact on Livelihood by 2040. Telemedicine, the use of technology to diagnose and treat patients remotely, has been rising in recent years. With the advent of high-speed internet and improved video conferencing tools, healthcare providers are increasingly turning to telemedicine to provide care to patients in remote or underserved areas.\n",
      "Telemedicine, using technology to provide healthcare services remotely, has recently gained popularity. With advancements in communication and medical technology, it has become increasingly possible for doctors and patients to connect and interact from anywhere in the world. This has led to the rise of telemedicine, which has the potential to revolutionize the way healthcare is delivered.#Telemedicine\n",
      "The increasing focus on preventative healthcare has also driven the rise of telemedicine. As more and more people become aware of the importance of staying healthy, they are looking for ways to prevent illness and maintain their health. Telemedicine allows people to monitor their health and get advice from healthcare providers without traveling to a clinic or hospital.\n",
      "Improved access to healthcare: One of the biggest challenges in the healthcare industry is the unequal distribution of medical resources, with many remote and rural areas lacking access to quality care. Telemedicine can help bridge this gap by allowing patients in these underserved areas to connect with healthcare providers worldwide. This can give people access to specialized medical care that they would otherwise not be able to receive, improving their health and overall well-being.#Healthcare\n",
      "Greater convenience and flexibility: Telemedicine also offers a level of convenience and flexibility that is impossible with traditional in-person medical visits. Patients can consult with their healthcare providers from the comfort of their own homes, saving time and money on travel. This is particularly beneficial for individuals who have mobility issues or who live in areas with limited access to medical facilities.\n",
      "Reduced healthcare costs: By enabling patients to receive care remotely, telemedicine can help reduce the overall cost of healthcare. This is because telemedicine visits are generally less expensive than in-person visits and can help prevent the need for more costly interventions, such as hospitalizations. As a result, telemedicine has the potential to make healthcare more affordable for patients and healthcare providers alike.#Costs\n",
      "Enhanced healthcare outcomes: Telemedicine can also improve patients’ quality of care. By allowing healthcare providers to access medical records and other important information remotely, telemedicine can help ensure that patients receive the most appropriate care for their needs. This can lead to better health outcomes and a higher quality of life for patients.#Enhanced\n",
      "More significant job opportunities in the healthcare industry like healthcare sector, including for healthcare providers, support staff, and technology experts. This can provide a boost to local economies and help to reduce unemployment.#Jobs\n",
      "Better care for chronic conditions: Telemedicine can help patients with chronic conditions to manage their health more effectively, allowing them to receive regular check-ups and treatment without the need to travel long distances. This can improve the quality of life for these individuals and reduce the risk of complications from their conditions.\n",
      "Increased flexibility for patients and healthcare providers: Telemedicine allows patients and providers to schedule consultations at times convenient for them, improving the overall flexibility of the healthcare system. This can help to reduce wait times and improve patient satisfaction.\n",
      "Increased accessibility: Telemedicine makes healthcare more accessible to people living in rural and remote areas, who may not have easy access to medical facilities. With telemedicine, patients can consult with doctors and specialists from their homes using their phones, computers, or other devices. This means that more people will be able to get the medical care they need, regardless of where they live.\n",
      "Reduced costs: One of the most significant benefits of telemedicine is that it can reduce healthcare costs. Patients can avoid the costs of travel, accommodation, and time off work associated with in-person visits to the doctor. Telemedicine can also help reduce the cost of medical procedures, as doctors can remotely monitor patients and provide care without needing hospitalization.\n",
      "Improved health outcomes: By providing patients with access to medical care from the comfort of their own homes, telemedicine can help improve their health outcomes. For example, patients with chronic conditions can use telemedicine to manage their symptoms and reduce the risk of complications. Telemedicine can also help prevent the spread of infectious diseases, as patients can receive care without contacting other people.\n",
      "Increased convenience: Telemedicine offers a more convenient option for patients who need medical care. With telemedicine, patients can schedule appointments and consult with doctors at a convenient time without having to take time off work or arrange transportation. This can be especially beneficial for people with busy schedules or mobility issues.\n",
      "Telemedicine, or the use of electronic communication and information technologies to provide healthcare services remotely, has the potential to significantly impact the way we receive and deliver medical care. In the next 20 years, telemedicine will likely become an increasingly important part of our healthcare system, providing numerous benefits and improving the livelihood of many people.\n",
      "The benefits of telemedicine are numerous. For patients, it provides convenient access to medical care without traveling long distances. It also allows for faster diagnosis and treatment and improved continuity of care. For healthcare providers, telemedicine can reduce the strain on already-overburdened healthcare systems and make providing care to patients in remote areas more accessible.\n",
      "The rise of telemedicine will likely significantly impact how we receive medical care in the future. By 2040, it is estimated that telemedicine will be a standard part of the healthcare landscape, with more and more doctors using it to diagnose and treat patients remotely.\n",
      "One of the key drivers of the rise of telemedicine has been the increasing availability of high-speed internet and improved video conferencing tools. In the past, the video and audio quality of telemedicine consultations were often poor, making it difficult for doctors to diagnose and treat patients accurately. However, with faster internet speeds and better video conferencing tools, telemedicine consultations are just as good as in-person visits.\n",
      "Telemedicine makes healthcare more accessible. With telemedicine, patients no longer travel long distances to see a doctor or specialist. Instead, they can receive care from the comfort of their own home. This is particularly beneficial for people living in rural areas, where access to healthcare can be limited.\n",
      "Another factor contributing to the rise of telemedicine is the increasing demand for healthcare services. With the global population growing and people living longer, the need for medical care will only continue to increase. Telemedicine offers a way for healthcare providers to meet this demand by allowing them to provide care to more patients in a shorter amount of time.\n",
      "Another substantial impact of telemedicine is that it can improve patient quality of care. By using technology to connect patients with doctors, telemedicine can help ensure patients receive the best care possible. For example, a doctor can use video conferencing to consult with a specialist or use remote monitoring devices to track a patient’s vital signs. This can help doctors make more informed decisions and provide better treatment.\n",
      "Telemedicine also has the potential to reduce healthcare costs. By allowing patients to receive care from their homes, telemedicine can reduce the need for expensive hospital visits. In addition, telemedicine can help prevent the spread of infectious diseases, reducing healthcare costs by avoiding hospitalization.\n",
      "One of the critical impacts of telemedicine is that it can improve the patient experience. By providing patients with the ability to receive care from the comfort of their own homes, telemedicine can help reduce anxiety and stress. In addition, telemedicine can make it easier for patients to communicate with their doctors and get the answers they need.\n",
      "Another way in which telemedicine will impact people’s livelihoods is by making healthcare more efficient. By allowing doctors to diagnose and treat patients remotely, telemedicine will help reduce the time patients spend waiting to see a doctor. It will also help reduce doctors’ spending on administrative tasks, freeing them up to see more patients and provide better care.\n",
      "One major factor driving the rise of telemedicine is the increasing use of technology in healthcare. With advances in communication and medical technology, it is now possible for doctors and other healthcare providers to remotely diagnose and treat patients using video conferencing, remote monitoring, and other forms of telemedicine. This not only allows for more efficient and convenient healthcare delivery but also enables healthcare providers to offer specialized care to patients who otherwise may not have access to it.\n",
      "Another factor contributing to the rise of telemedicine is the need for improved access to healthcare. Patients may travel long distances in many rural and underserved areas; patients may travel long distances to access medical care. Telemedicine can bridge this gap by allowing patients in these areas to consult with healthcare providers remotely. This not only improves access to healthcare but also reduces the need for patients to travel long distances, saving them time and money.\n",
      "Finally, telemedicine’s rise is also driven by the growing trend of remote work. As more and more people work from home or other remote locations, the need for convenient and accessible healthcare is increasing. Telemedicine allows employees to consult with healthcare providers from their homes, reducing the need for them to take time off work to visit a doctor or other healthcare provider.\n",
      "Overall, the rise of telemedicine by 2040 is a result of a combination of factors, including the increasing use of technology in healthcare, the need for improved access to healthcare, and the growing trend of remote work. As these factors continue to evolve, likely, telemedicine will likely become an increasingly important part of the healthcare landscape.\n",
      "However, the rise of telemedicine is also likely to present some challenges. For example, telemedicine consultations may only be suitable for some medical conditions, and some patients may still need to see a doctor. Additionally, telemedicine may put some jobs at risk, such as receptionists and other administrative staff in healthcare facilities.\n",
      "Blackcoffer Insights 46: G.K.Harish Balaji, Indian Institute of Technology,Madras\n"
     ]
    }
   ],
   "source": [
    "file = 'output_data/123.0.txt'\n",
    "with open(file, \"r\", encoding='utf-8') as read_file:\n",
    "    reader = read_file.read()\n",
    "\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lower\n",
    "pattern = r'[0-9]'\n",
    "text = re.sub(pattern,'', reader).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Sentence length\n",
    "avg number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_into_sentences(text):\n",
    "    sen = spacy.load('en_core_web_sm')\n",
    "    doc_text = sen(text)\n",
    "    return list(doc_text.sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.8"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_sentence_length(text):\n",
    "    \n",
    "    regular_punct = list(string.punctuation)\n",
    "\n",
    "    sentences_list = break_into_sentences(text)\n",
    "    word_count = 0\n",
    "    for sentence in sentences_list:\n",
    "       list2 = [str(token) for token  in sentence]\n",
    "\n",
    "       for punc_element in list2:\n",
    "            for punc in regular_punct:\n",
    "                  if punc == punc_element:\n",
    "                        list2.remove(punc_element)\n",
    "\n",
    "       element_to_remove = '\\n'\n",
    "       list2 = [i for i in list2 if i != element_to_remove]\n",
    "\n",
    "       word_count += len(list2)\n",
    "\n",
    "    len_sentences = len(break_into_sentences(text))\n",
    "    average_sentence_length = (word_count / len_sentences)\n",
    "    return round(average_sentence_length,2)\n",
    "\n",
    "avg_sentence_length(text)\n",
    "\n",
    "# print(f'{avg_sen_len} {word_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1685\n"
     ]
    }
   ],
   "source": [
    "def word_count_raw_text(text):\n",
    "    \n",
    "    regular_punct = list(string.punctuation)\n",
    "\n",
    "    sentences_list = break_into_sentences(text)\n",
    "    word_list = []\n",
    "    for sentence in sentences_list:\n",
    "       list2 = [str(token) for token  in sentence]\n",
    "\n",
    "       for punc_element in list2:\n",
    "            for punc in regular_punct:\n",
    "                  if punc == punc_element:\n",
    "                        list2.remove(punc_element)\n",
    "\n",
    "       element_to_remove = '\\n'\n",
    "       list2 = [i for i in list2 if i != element_to_remove]\n",
    "       \n",
    "       word_list += list2\n",
    "    return word_list\n",
    "\n",
    "word_list= word_count_raw_text(text)\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuations\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    regular_punct = list(string.punctuation)\n",
    "    for punc in regular_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, '')\n",
    "    punc_text = text.strip()\n",
    "     \n",
    "    filtered_text = re.sub(r\"http\\S+\", \"\", punc_text)\n",
    "    return filtered_text\n",
    "\n",
    "filter_text = remove_punctuation(text) # processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "def tokenize(text):\n",
    "    text_tokenize = word_tokenize(text)\n",
    "    return text_tokenize\n",
    "    \n",
    "tokenized_text = tokenize(filter_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676\n"
     ]
    }
   ],
   "source": [
    "def word_count(tokienized_text):\n",
    "    lent = len(tokienized_text)\n",
    "    return lent\n",
    "\n",
    "text_tokenize = word_count(tokenize(filter_text))\n",
    "print(text_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high\n",
      "speed\n",
      "delivered.#telemedicine\n",
      "well\n",
      "being.#healthcare\n",
      "person\n",
      "person\n",
      "alike.#costs\n",
      "patients.#enhanced\n",
      "unemployment.#jobs\n",
      "check\n",
      "ups\n",
      "person\n",
      " \n",
      "already\n",
      "overburdened\n",
      "high\n",
      "speed\n",
      "person\n",
      "’s\n",
      "’s\n",
      "’s\n",
      " \n",
      "g.k.harish\n",
      "madras\n"
     ]
    }
   ],
   "source": [
    "# Words that are there in word_list but not in the tokenize result. Since the, words are neutral \n",
    "# and only 10 in number with no precise meaning ... we will ignore these. And will go with the word \n",
    "# count of tokenize \n",
    "\n",
    "for k in word_list:\n",
    "    if k not in tokenized_text:\n",
    "        print(k) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.58"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word_length(tokenized_text):\n",
    "  sum_word_length = 0\n",
    "  for element in tokenized_text:  \n",
    "    sum_word_length += len(element)\n",
    "    avg_length = sum_word_length / len(tokenized_text)\n",
    "  return round(avg_length,2)\n",
    "\n",
    "avg_word_length(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "def remove_stop_words(tokenized_text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in tokenized_text if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "def lemmatize(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words_lemmatized\n",
    "\n",
    "words = remove_stop_words(tokenized_text)\n",
    "words_lemmatized = lemmatize(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSITIVE SCORE/ NEGATIVE SCORE / POLARITY SCORE/ SUBJECTIVITY SCORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00014\n",
      "0.05\n",
      "0.12\n",
      "0.02\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analyzer(words, words_lemmatized):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = [sia.polarity_scores(word)['compound'] for word in words_lemmatized]\n",
    "    average_sentiment = sum(sentiment_score) / len(sentiment_score)\n",
    "    positive_words = [word for i, word in enumerate(words) if sentiment_score[i] >= 0.05]\n",
    "    negative_words = [word for i, word in enumerate(words) if sentiment_score[i] <= -0.05]\n",
    "\n",
    "    total_words = len(words)\n",
    "    positive_score = (len(positive_words) / total_words) \n",
    "    negative_score = (len(negative_words) / total_words)\n",
    "    subjectivity_score = (positive_score + negative_score)/ ((total_words) + 0.000001) #Range is from 0 to +1\n",
    "    return round(average_sentiment,2), round(positive_score,2), round(negative_score,2), '%.5f' % subjectivity_score\n",
    "\n",
    "x, y, z, k = sentiment_analyzer(words, words_lemmatized)\n",
    "print(k)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Sylable per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def syllables_count(filter_text):\n",
    "    return textstatistics().syllable_count(filter_text)\n",
    " \n",
    "# Returns the average number of syllables per\n",
    "# word in the text\n",
    "def avg_syllables_per_word(text, tokenized_text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = len(tokenized_text)\n",
    "    avg_syllable = syllable / words\n",
    "    return round(avg_syllable,2)\n",
    "\n",
    "avg_syllables_per_word(filter_text,tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "24.49 %\n"
     ]
    }
   ],
   "source": [
    "def difficult_words(tokenized_text):\n",
    "     \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # doc = nlp(text)\n",
    " \n",
    "    # difficult words are those with syllables >= 2\n",
    "    # easy_word_set is provide by Textstat as\n",
    "    # a list of common words\n",
    "    diff_words_set = set()\n",
    "     \n",
    "    for word in tokenized_text:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    " \n",
    "    return len(diff_words_set), round(((len(diff_words_set)/ len(tokenized_text))*100),2)\n",
    "\n",
    "total_complex_words, complex_percentage= difficult_words(tokenized_text)\n",
    "\n",
    "print(total_complex_words)\n",
    "print(complex_percentage,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/its/they/them/we/our'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_pronouns(text):\n",
    "    find_pronouns = re.compile(r'\\b(I|you|he|she|it|we|they|them|him|her|his|hers|its|theirs|our|your(?-i:us))\\b',re.I)\n",
    "    pronouns = find_pronouns.findall(text)\n",
    "\n",
    "    final_pronouns = \"\"\n",
    "    for element in pronouns:\n",
    "        if element not in final_pronouns:\n",
    "            final_pronouns = final_pronouns + \"/\" + element\n",
    "            \n",
    "    return final_pronouns\n",
    "\n",
    "get_pronouns(filter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.94"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flesch_reading_ease(text):\n",
    "    \"\"\"\n",
    "       Flesch Formula = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "        Here,\n",
    "          ASL = average sentence length (number of words divided by number of sentences)\n",
    "          ASW = average word length in syllables (number of syllables divided by number of words)\n",
    "    \"\"\"\n",
    "    FRE = 206.835 - (1.015 * avg_sentence_length(text)) - (84.6 * avg_syllables_per_word(text))\n",
    "    return round(FRE,2)\n",
    "\n",
    "flesch_reading_ease(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions are ready! Now we have to loop over 112 text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_into_sentences(text) # will work on raw text\n",
    "avg_sentence_length(text) # will work on raw text\n",
    "remove_punctuation(text) # will work on raw text > # filtered_text\n",
    "tokenize(text) # will work on filtered_text > # tokenized_text\n",
    "word_count(tokienized_text) # will work on tokenized_text\n",
    "avg_word_length(tokenized_text) # will work on tokenized_text\n",
    "remove_stop_words(tokenized_text) # will work on tokenized_text > # words\n",
    "lemmatize(words) # words_lemmatized\n",
    "sentiment_analyzer(words, words_lemmatized)\n",
    "syllables_count(text) # will work on filter_text\n",
    "avg_syllables_per_word(text)  # will work on filter_text\n",
    "difficult_words(tokenized_text) # will work on tokenized_text\n",
    "get_pronouns(text) # will work on filter_text\n",
    "flesch_reading_ease(text) # will work on raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_url_df = pd.read_csv('input.csv')\n",
    "raw_df = input_url_df[['URL_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File under progress: 10282.6.txt ...\n",
      "File under progress: 10744.4.txt ...\n",
      "File under progress: 11206.2.txt ...\n",
      "File under progress: 12129.8.txt ...\n",
      "File under progress: 123.0.txt ...\n",
      "File under progress: 12591.6.txt ...\n",
      "File under progress: 13053.4.txt ...\n",
      "File under progress: 13515.2.txt ...\n",
      "File under progress: 13977.0.txt ...\n",
      "File under progress: 14438.8.txt ...\n",
      "File under progress: 14900.6.txt ...\n",
      "File under progress: 15362.4.txt ...\n",
      "File under progress: 15824.2.txt ...\n",
      "File under progress: 16286.0.txt ...\n",
      "File under progress: 16747.8.txt ...\n",
      "File under progress: 17209.6.txt ...\n",
      "File under progress: 18133.2.txt ...\n",
      "File under progress: 18595.0.txt ...\n",
      "File under progress: 19056.8.txt ...\n",
      "File under progress: 19518.6.txt ...\n",
      "File under progress: 19980.4.txt ...\n",
      "File under progress: 20442.2.txt ...\n",
      "File under progress: 20904.0.txt ...\n",
      "File under progress: 21365.8.txt ...\n",
      "File under progress: 21827.6.txt ...\n",
      "File under progress: 22289.4.txt ...\n",
      "File under progress: 22751.2.txt ...\n",
      "File under progress: 23213.0.txt ...\n",
      "File under progress: 2345.0.txt ...\n",
      "File under progress: 23674.8.txt ...\n",
      "File under progress: 24136.6.txt ...\n",
      "File under progress: 24598.4.txt ...\n",
      "File under progress: 25060.2.txt ...\n",
      "File under progress: 25522.0.txt ...\n",
      "File under progress: 25983.8.txt ...\n",
      "File under progress: 26445.6.txt ...\n",
      "File under progress: 26907.4.txt ...\n",
      "File under progress: 27369.2.txt ...\n",
      "File under progress: 27831.0.txt ...\n",
      "File under progress: 28292.8.txt ...\n",
      "File under progress: 28754.6.txt ...\n",
      "File under progress: 2893.8.txt ...\n",
      "File under progress: 29216.4.txt ...\n",
      "File under progress: 29678.2.txt ...\n",
      "File under progress: 30140.0.txt ...\n",
      "File under progress: 30601.8.txt ...\n",
      "File under progress: 31063.6.txt ...\n",
      "File under progress: 31525.4.txt ...\n",
      "File under progress: 31987.2.txt ...\n",
      "File under progress: 321.0.txt ...\n",
      "File under progress: 32449.0.txt ...\n",
      "File under progress: 32910.8.txt ...\n",
      "File under progress: 33372.6.txt ...\n",
      "File under progress: 3355.6.txt ...\n",
      "File under progress: 33834.4.txt ...\n",
      "File under progress: 34296.2.txt ...\n",
      "File under progress: 34758.0.txt ...\n",
      "File under progress: 35219.8.txt ...\n",
      "File under progress: 35681.6.txt ...\n",
      "File under progress: 36143.4.txt ...\n",
      "File under progress: 36605.2.txt ...\n",
      "File under progress: 37067.0.txt ...\n",
      "File under progress: 37528.8.txt ...\n",
      "File under progress: 37990.6.txt ...\n",
      "File under progress: 3817.4.txt ...\n",
      "File under progress: 38452.4.txt ...\n",
      "File under progress: 38914.2.txt ...\n",
      "File under progress: 39376.0.txt ...\n",
      "File under progress: 39837.8.txt ...\n",
      "File under progress: 40299.6.txt ...\n",
      "File under progress: 40761.4.txt ...\n",
      "File under progress: 41223.2.txt ...\n",
      "File under progress: 41685.0.txt ...\n",
      "File under progress: 42146.8.txt ...\n",
      "File under progress: 42608.6.txt ...\n",
      "File under progress: 4279.2.txt ...\n",
      "File under progress: 43070.4.txt ...\n",
      "File under progress: 432.0.txt ...\n",
      "File under progress: 4321.0.txt ...\n",
      "File under progress: 43532.2.txt ...\n",
      "File under progress: 43994.0.txt ...\n",
      "File under progress: 44455.8.txt ...\n",
      "File under progress: 44917.6.txt ...\n",
      "File under progress: 45379.4.txt ...\n",
      "File under progress: 45841.2.txt ...\n",
      "File under progress: 46303.0.txt ...\n",
      "File under progress: 46764.8.txt ...\n",
      "File under progress: 47226.6.txt ...\n",
      "File under progress: 4741.0.txt ...\n",
      "File under progress: 47688.4.txt ...\n",
      "File under progress: 48150.2.txt ...\n",
      "File under progress: 48612.0.txt ...\n",
      "File under progress: 49073.8.txt ...\n",
      "File under progress: 49535.6.txt ...\n",
      "File under progress: 49997.4.txt ...\n",
      "File under progress: 50459.2.txt ...\n",
      "File under progress: 50921.0.txt ...\n",
      "File under progress: 51382.8.txt ...\n",
      "File under progress: 51844.6.txt ...\n",
      "File under progress: 5202.8.txt ...\n",
      "File under progress: 52306.4.txt ...\n",
      "File under progress: 52768.2.txt ...\n",
      "File under progress: 5664.6.txt ...\n",
      "File under progress: 6126.4.txt ...\n",
      "File under progress: 6588.2.txt ...\n",
      "File under progress: 7050.0.txt ...\n",
      "File under progress: 7511.8.txt ...\n",
      "File under progress: 7973.6.txt ...\n",
      "File under progress: 8435.4.txt ...\n",
      "File under progress: 8897.2.txt ...\n",
      "File under progress: 9359.0.txt ...\n",
      "File under progress: 9820.8.txt ...\n"
     ]
    }
   ],
   "source": [
    "path = 'output_data/'\n",
    "raw_temp = raw_df\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    print(f'File under progress: {file} ...')\n",
    "    \n",
    "    url_id = float(file.split('.txt')[0])\n",
    "    \n",
    "    try: \n",
    "        with open(os.path.join(path,file), \"r\", encoding='utf-8') as info_file:\n",
    "            reader = info_file.read()\n",
    "        \n",
    "        pattern = r'[0-9]'\n",
    "        text = re.sub(pattern,'', reader).lower()\n",
    "\n",
    "        break_into_sentences(text)\n",
    "        avg_sent_length = avg_sentence_length(text)\n",
    "\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Avg_sentence_length'] = avg_sent_length\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Avg_number_of_words_per_sentence'] = avg_sent_length\n",
    "\n",
    "        filtered_text = remove_punctuation(text)\n",
    "        tokenized_text = tokenize(filtered_text)\n",
    "        word_counts = word_count(tokenized_text)\n",
    "\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Word_count'] = int(word_counts)\n",
    "\n",
    "        avg_word_len = avg_word_length(tokenized_text)\n",
    "\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Avg_word_length'] = int(avg_word_len)\n",
    "\n",
    "        words = remove_stop_words(tokenized_text)\n",
    "        words_lemmatized = lemmatize(words)\n",
    "        polar, pos, neg, sub = sentiment_analyzer(words, words_lemmatized)\n",
    "\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Positive_score'] = pos\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Negative_score'] = neg\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Polarity_score'] = polar\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Subjectivity_score'] = sub\n",
    "\n",
    "        syllables_count(filtered_text)\n",
    "        avg_syllable_per_word = avg_syllables_per_word(filtered_text)\n",
    "\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Avg_syllables_per_word'] = avg_syllable_per_word\n",
    "\n",
    "        complex_count, complex_word_percent = difficult_words(tokenized_text)\n",
    "\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Complex_word_count'] = int(complex_count)\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Complex_word_percentage'] = complex_word_percent\n",
    "\n",
    "        pronouns = get_pronouns(text) \n",
    "\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Personal_pronouns'] = pronouns\n",
    "\n",
    "        # FRE = flesch_reading_ease(text)\n",
    "        fog_index = 0.4*(avg_sent_length + complex_word_percent)\n",
    "\n",
    "        raw_temp.loc[(raw_temp['URL_ID']==url_id), 'Fog_index'] = fog_index\n",
    "\n",
    "    except Exception as e :\n",
    "            print(str(e))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>Avg_sentence_length</th>\n",
       "      <th>Avg_number_of_words_per_sentence</th>\n",
       "      <th>Word_count</th>\n",
       "      <th>Avg_word_length</th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "      <th>Polarity_score</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Avg_syllables_per_word</th>\n",
       "      <th>Complex_word_count</th>\n",
       "      <th>Complex_word_percentage</th>\n",
       "      <th>Personal_pronouns</th>\n",
       "      <th>Fog_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.80</td>\n",
       "      <td>1676.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>1.64</td>\n",
       "      <td>237.0</td>\n",
       "      <td>14.14</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>13.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>24.38</td>\n",
       "      <td>24.38</td>\n",
       "      <td>617.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00042</td>\n",
       "      <td>1.72</td>\n",
       "      <td>134.0</td>\n",
       "      <td>21.72</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>18.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>16.25</td>\n",
       "      <td>16.25</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00019</td>\n",
       "      <td>1.64</td>\n",
       "      <td>247.0</td>\n",
       "      <td>22.91</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>15.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>22.82</td>\n",
       "      <td>22.82</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>1.65</td>\n",
       "      <td>324.0</td>\n",
       "      <td>26.00</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>19.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>22.82</td>\n",
       "      <td>22.82</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>1.65</td>\n",
       "      <td>324.0</td>\n",
       "      <td>26.00</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>19.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>50921.0</td>\n",
       "      <td>25.11</td>\n",
       "      <td>25.11</td>\n",
       "      <td>658.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>1.63</td>\n",
       "      <td>172.0</td>\n",
       "      <td>26.14</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>20.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>51382.8</td>\n",
       "      <td>33.38</td>\n",
       "      <td>33.38</td>\n",
       "      <td>1676.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>1.62</td>\n",
       "      <td>341.0</td>\n",
       "      <td>20.35</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>21.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>51844.6</td>\n",
       "      <td>26.36</td>\n",
       "      <td>26.36</td>\n",
       "      <td>1781.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>1.60</td>\n",
       "      <td>444.0</td>\n",
       "      <td>24.93</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>20.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>52306.4</td>\n",
       "      <td>25.07</td>\n",
       "      <td>25.07</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>1.51</td>\n",
       "      <td>271.0</td>\n",
       "      <td>19.37</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>17.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>52768.2</td>\n",
       "      <td>24.68</td>\n",
       "      <td>24.68</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>1.81</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.72</td>\n",
       "      <td>/its/they/them/we/our</td>\n",
       "      <td>22.160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID  Avg_sentence_length  Avg_number_of_words_per_sentence  \\\n",
       "0      123.0                20.80                             20.80   \n",
       "1      321.0                24.38                             24.38   \n",
       "2     2345.0                16.25                             16.25   \n",
       "3     4321.0                22.82                             22.82   \n",
       "4      432.0                22.82                             22.82   \n",
       "..       ...                  ...                               ...   \n",
       "109  50921.0                25.11                             25.11   \n",
       "110  51382.8                33.38                             33.38   \n",
       "111  51844.6                26.36                             26.36   \n",
       "112  52306.4                25.07                             25.07   \n",
       "113  52768.2                24.68                             24.68   \n",
       "\n",
       "     Word_count  Avg_word_length  Positive_score  Negative_score  \\\n",
       "0        1676.0              5.0            0.12            0.02   \n",
       "1         617.0              5.0            0.13            0.01   \n",
       "2        1078.0              5.0            0.08            0.04   \n",
       "3        1246.0              5.0            0.12            0.03   \n",
       "4        1246.0              5.0            0.12            0.03   \n",
       "..          ...              ...             ...             ...   \n",
       "109       658.0              5.0            0.02            0.04   \n",
       "110      1676.0              5.0            0.04            0.09   \n",
       "111      1781.0              5.0            0.10            0.02   \n",
       "112      1399.0              4.0            0.06            0.03   \n",
       "113      1009.0              5.0            0.12            0.06   \n",
       "\n",
       "     Polarity_score Subjectivity_score  Avg_syllables_per_word  \\\n",
       "0              0.05            0.00014                    1.64   \n",
       "1              0.05            0.00042                    1.72   \n",
       "2              0.02            0.00019                    1.64   \n",
       "3              0.03            0.00020                    1.65   \n",
       "4              0.03            0.00020                    1.65   \n",
       "..              ...                ...                     ...   \n",
       "109           -0.01            0.00015                    1.63   \n",
       "110           -0.01            0.00012                    1.62   \n",
       "111            0.03            0.00012                    1.60   \n",
       "112            0.01            0.00011                    1.51   \n",
       "113            0.02            0.00028                    1.81   \n",
       "\n",
       "     Complex_word_count  Complex_word_percentage      Personal_pronouns  \\\n",
       "0                 237.0                    14.14  /its/they/them/we/our   \n",
       "1                 134.0                    21.72  /its/they/them/we/our   \n",
       "2                 247.0                    22.91  /its/they/them/we/our   \n",
       "3                 324.0                    26.00  /its/they/them/we/our   \n",
       "4                 324.0                    26.00  /its/they/them/we/our   \n",
       "..                  ...                      ...                    ...   \n",
       "109               172.0                    26.14  /its/they/them/we/our   \n",
       "110               341.0                    20.35  /its/they/them/we/our   \n",
       "111               444.0                    24.93  /its/they/them/we/our   \n",
       "112               271.0                    19.37  /its/they/them/we/our   \n",
       "113               310.0                    30.72  /its/they/them/we/our   \n",
       "\n",
       "     Fog_index  \n",
       "0       13.976  \n",
       "1       18.440  \n",
       "2       15.664  \n",
       "3       19.528  \n",
       "4       19.528  \n",
       "..         ...  \n",
       "109     20.500  \n",
       "110     21.492  \n",
       "111     20.516  \n",
       "112     17.776  \n",
       "113     22.160  \n",
       "\n",
       "[114 rows x 14 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.merge(input_url_df, raw_temp, on = 'URL_ID', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL_ID                              0\n",
       "URL                                 0\n",
       "Avg_sentence_length                 2\n",
       "Avg_number_of_words_per_sentence    2\n",
       "Word_count                          2\n",
       "Avg_word_length                     2\n",
       "Positive_score                      2\n",
       "Negative_score                      2\n",
       "Polarity_score                      2\n",
       "Subjectivity_score                  2\n",
       "Avg_syllables_per_word              2\n",
       "Complex_word_count                  2\n",
       "Complex_word_percentage             2\n",
       "Personal_pronouns                   2\n",
       "Fog_index                           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.isna().sum() # for those two url's whose webpage doesn't exit ...values are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.fillna('Not available', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Avg_sentence_length</th>\n",
       "      <th>Avg_number_of_words_per_sentence</th>\n",
       "      <th>Word_count</th>\n",
       "      <th>Avg_word_length</th>\n",
       "      <th>Positive_score</th>\n",
       "      <th>Negative_score</th>\n",
       "      <th>Polarity_score</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Avg_syllables_per_word</th>\n",
       "      <th>Complex_word_count</th>\n",
       "      <th>Complex_word_percentage</th>\n",
       "      <th>Personal_pronouns</th>\n",
       "      <th>Fog_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11668.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-neural-ne...</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>17671.4</td>\n",
       "      <td>https://insights.blackcoffer.com/covid-19-envi...</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "24  11668.0  https://insights.blackcoffer.com/how-neural-ne...   \n",
       "37  17671.4  https://insights.blackcoffer.com/covid-19-envi...   \n",
       "\n",
       "   Avg_sentence_length Avg_number_of_words_per_sentence     Word_count  \\\n",
       "24       Not available                    Not available  Not available   \n",
       "37       Not available                    Not available  Not available   \n",
       "\n",
       "   Avg_word_length Positive_score Negative_score Polarity_score  \\\n",
       "24   Not available  Not available  Not available  Not available   \n",
       "37   Not available  Not available  Not available  Not available   \n",
       "\n",
       "   Subjectivity_score Avg_syllables_per_word Complex_word_count  \\\n",
       "24      Not available          Not available      Not available   \n",
       "37      Not available          Not available      Not available   \n",
       "\n",
       "   Complex_word_percentage Personal_pronouns      Fog_index  \n",
       "24           Not available     Not available  Not available  \n",
       "37           Not available     Not available  Not available  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[output_df['Avg_sentence_length'] == 'Not available']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert df to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_excel(r'F:/My_Disk/python/Projects/NLP/outptut.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
